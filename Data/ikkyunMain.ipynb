{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "707a5c74",
   "metadata": {},
   "source": [
    "## ğŸ¾ OpenAI API í‚¤ ë°œê¸‰ ë° ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55cba5aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# API KEYë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê¸° ìœ„í•œ ì„¤ì • íŒŒì¼\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# API KEY ì •ë³´ë¡œë“œ\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ce4f545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[API KEY]\n",
      "sk-proj-y4WToRFYqZ-Wj0gO27S4wqQdFi4npu7mKV5RFHUEcIBuE_riXH0aLGMjXEHMztDkz0VQc1I0hsT3BlbkFJq0VCbNlLt9D-MkAmmk_8eyCar687htSINT9IDo93U4--tA7Vzefg0tpbh9s***************\n"
     ]
    }
   ],
   "source": [
    "# API KEY ì •ë³´ í™•ì¸\n",
    "import os\n",
    "\n",
    "print(f\"[API KEY]\\n{os.environ['OPENAI_API_KEY'][:-15]}\" + \"*\" * 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c02d0483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LangChain ê´€ë ¨ íŒ¨í‚¤ì§€ ë²„ì „]\n",
      "langchain: 0.3.23\n",
      "langchain-core: 0.3.51\n",
      "langchain-experimental: 0.3.4\n",
      "langchain-community: 0.3.21\n",
      "langchain-openai: 0.3.12\n",
      "langchain-teddynote: 0.3.45\n",
      "langchain-huggingface: 0.1.2\n",
      "langchain-google-genai: 2.1.2\n",
      "langchain-anthropic: 0.3.10\n",
      "langchain-cohere: 0.4.3\n",
      "langchain-chroma: 0.2.2\n",
      "langchain-elasticsearch: 0.3.2\n",
      "langchain-upstage: ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ\n",
      "langchain-cohere: 0.4.3\n",
      "langchain-milvus: 0.1.9\n",
      "langchain-text-splitters: 0.3.8\n"
     ]
    }
   ],
   "source": [
    "# LangCahin ë²„ì „ í™•ì¸\n",
    "from importlib.metadata import version\n",
    "\n",
    "print(\"[LangChain ê´€ë ¨ íŒ¨í‚¤ì§€ ë²„ì „]\")\n",
    "for package_name in [\n",
    "    \"langchain\",\n",
    "    \"langchain-core\",\n",
    "    \"langchain-experimental\",\n",
    "    \"langchain-community\",\n",
    "    \"langchain-openai\",\n",
    "    \"langchain-teddynote\",\n",
    "    \"langchain-huggingface\",\n",
    "    \"langchain-google-genai\",\n",
    "    \"langchain-anthropic\",\n",
    "    \"langchain-cohere\",\n",
    "    \"langchain-chroma\",\n",
    "    \"langchain-elasticsearch\",\n",
    "    \"langchain-upstage\",\n",
    "    \"langchain-cohere\",\n",
    "    \"langchain-milvus\",\n",
    "    \"langchain-text-splitters\",\n",
    "]:\n",
    "    try:\n",
    "        package_version = version(package_name)\n",
    "        print(f\"{package_name}: {package_version}\")\n",
    "    except ImportError:\n",
    "        print(f\"{package_name}: ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8093faf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith ì¶”ì ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
      "[í”„ë¡œì íŠ¸ëª…]\n",
      "PromptIkkyunFinal\n"
     ]
    }
   ],
   "source": [
    "# # LangSmith ì¶”ì  ì„¤ì • https://smith.langchain.com\n",
    "# # .env íŒŒì¼ì— LANGCHAIN_API_KEY ì…ë ¥\n",
    "# # !pip install -qU langchain-teddynote\n",
    "# from langchain_teddynote import logging\n",
    "\n",
    "# # í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥\n",
    "# logging.langsmith(\"***\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e913cd91",
   "metadata": {},
   "source": [
    "## ğŸ» ë°ì´í„°ì…‹ 1000ê°œ ìµœì¢…ver\n",
    "ë¶€ì • ë¦¬ë·° 500ê°œ (1000 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6551a10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `_type` key found, defaulting to `prompt`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì§„í–‰ ìƒí™©: 0 / 500 ë¬¸ì¥ ì²˜ë¦¬ ì™„ë£Œ\n",
      "ì§„í–‰ ìƒí™©: 100 / 500 ë¬¸ì¥ ì²˜ë¦¬ ì™„ë£Œ\n",
      "ì§„í–‰ ìƒí™©: 200 / 500 ë¬¸ì¥ ì²˜ë¦¬ ì™„ë£Œ\n",
      "ì§„í–‰ ìƒí™©: 300 / 500 ë¬¸ì¥ ì²˜ë¦¬ ì™„ë£Œ\n",
      "ì§„í–‰ ìƒí™©: 400 / 500 ë¬¸ì¥ ì²˜ë¦¬ ì™„ë£Œ\n",
      "ì €ì¥ ì™„ë£Œ: Ikkyun_1000.jsonl / Ikkyun_1000.csv\n",
      "ì´ ìƒ˜í”Œ ìˆ˜: 1000ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ\n",
    "import pandas as pd\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import load_prompt\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# .env ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# ë³€ê²½ëœ Pydantic ì¶œë ¥ ëª¨ë¸ ì •ì˜\n",
    "class SarcasmPair(BaseModel):\n",
    "    context: str = Field(description=\"ë¦¬ë·° ë‚´ìš©ì„ ìš”ì•½í•œ ë¬¸ë§¥\")\n",
    "    sarcastic_version: str = Field(description=\"í’ìì ìœ¼ë¡œ ì¬ì‘ì„±ëœ ë¬¸ì¥\")\n",
    "    non_sarcastic_version: str = Field(description=\"í’ì ì—†ì´ ì‚¬ì‹¤ë§Œì„ ì „ë‹¬í•œ ë¬¸ì¥\")\n",
    "    sarcastic_explanation: str = Field(\n",
    "        description=\"í’ì ë¬¸ì¥ì´ ì™œ í’ìì¸ì§€ì— ëŒ€í•œ ì„¤ëª…\"\n",
    "    )\n",
    "    non_sarcastic_explanation: str = Field(\n",
    "        description=\"ë¹„í’ì ë¬¸ì¥ì´ ì™œ í’ìê°€ ì•„ë‹Œì§€ì— ëŒ€í•œ ì„¤ëª…\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Output parser\n",
    "parser = PydanticOutputParser(pydantic_object=SarcasmPair)\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ë¡œë”©\n",
    "prompt = load_prompt(\"sarcasm_prompt.yaml\", encoding=\"utf-8\")\n",
    "\n",
    "# ëª¨ë¸ ì„¤ì •\n",
    "llm_4o = ChatOpenAI(temperature=0.5, model_name=\"gpt-4o\")\n",
    "llm_mini = ChatOpenAI(temperature=0.5, model_name=\"gpt-4o-mini\")\n",
    "\n",
    "chain_4o = prompt | llm_4o | parser\n",
    "chain_mini = prompt | llm_mini | parser\n",
    "\n",
    "# ë¦¬ë·° 500ê°œ ë¡œë“œ\n",
    "df = pd.read_csv(\"negative_reviews.csv\")\n",
    "sample_df = df.sample(n=500, random_state=777).reset_index(drop=True)\n",
    "\n",
    "# ì‹¤í–‰\n",
    "results = []\n",
    "for idx, text in enumerate(sample_df[\"text\"]):\n",
    "    model_chain = chain_4o if len(text) <= 50 else chain_mini\n",
    "    try:\n",
    "        result = model_chain.invoke({\"original\": text})\n",
    "\n",
    "        # í’ì ë²„ì „\n",
    "        results.append(\n",
    "            {\n",
    "                \"context\": result.context,\n",
    "                \"response\": result.sarcastic_version,\n",
    "                \"label\": \"Sarcasm\",\n",
    "                \"explanation\": result.sarcastic_explanation,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # ë¹„í’ì ë²„ì „\n",
    "        results.append(\n",
    "            {\n",
    "                \"context\": result.context,\n",
    "                \"response\": result.non_sarcastic_version,\n",
    "                \"label\": \"Non-Sarcasm\",\n",
    "                \"explanation\": result.non_sarcastic_explanation,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"ì§„í–‰ ìƒí™©: {idx} / 500 ë¬¸ì¥ ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[âš ï¸] ì˜¤ë¥˜ ë°œìƒ - {text[:30]}... â†’ {e}\")\n",
    "        continue\n",
    "\n",
    "# ì €ì¥\n",
    "jsonl_path = \"review_train.jsonl\"\n",
    "csv_path = \"review_train.csv\"\n",
    "\n",
    "with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in results:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "pd.DataFrame(results).to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"ì €ì¥ ì™„ë£Œ: {jsonl_path} / {csv_path}\")\n",
    "print(f\"ì´ ìƒ˜í”Œ ìˆ˜: {len(results)}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4818bb4b",
   "metadata": {},
   "source": [
    "### ê²°ê³¼ìš”ì•½\n",
    "\n",
    "í† í° ì‚¬ìš©ëŸ‰: 373,209  \n",
    "ë¹„ìš©: $1.05  \n",
    "ëŸ°íƒ€ì„: 28ë¶„  \n",
    "ë¹„ê³ : í’ˆì§ˆ ê´œì°®ìŒ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da70c2b",
   "metadata": {},
   "source": [
    "## ğŸ¼ í…ŒìŠ¤íŠ¸ì…‹ 200ê°œ\n",
    "íŠ¸ë ˆì´ë‹ì…‹ê³¼ ê²¹ì¹˜ì§€ ì•ŠëŠ” ë¶€ì • ë¦¬ë·° 100ê°œ (200 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9915b454",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `_type` key found, defaulting to `prompt`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[í…ŒìŠ¤íŠ¸ì…‹ ì§„í–‰ ìƒí™©] 0 / 100 ë¦¬ë·° ì²˜ë¦¬ ì™„ë£Œ\n",
      "[í…ŒìŠ¤íŠ¸ì…‹ ì§„í–‰ ìƒí™©] 20 / 100 ë¦¬ë·° ì²˜ë¦¬ ì™„ë£Œ\n",
      "[í…ŒìŠ¤íŠ¸ì…‹ ì§„í–‰ ìƒí™©] 40 / 100 ë¦¬ë·° ì²˜ë¦¬ ì™„ë£Œ\n",
      "[í…ŒìŠ¤íŠ¸ì…‹ ì§„í–‰ ìƒí™©] 60 / 100 ë¦¬ë·° ì²˜ë¦¬ ì™„ë£Œ\n",
      "[í…ŒìŠ¤íŠ¸ì…‹ ì§„í–‰ ìƒí™©] 80 / 100 ë¦¬ë·° ì²˜ë¦¬ ì™„ë£Œ\n",
      "í…ŒìŠ¤íŠ¸ì…‹ ì €ì¥ ì™„ë£Œ: Ikkyun_test200.jsonl / Ikkyun_test200.csv\n",
      "ì´ ìƒ˜í”Œ ìˆ˜: 200ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ\n",
    "import pandas as pd\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import load_prompt\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# .env ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# ì¶œë ¥ ëª¨ë¸ ì •ì˜\n",
    "class SarcasmPair(BaseModel):\n",
    "    context: str = Field(description=\"ë¦¬ë·° ë‚´ìš©ì„ ìš”ì•½í•œ ë¬¸ë§¥\")\n",
    "    sarcastic_version: str = Field(description=\"í’ìì ìœ¼ë¡œ ì¬ì‘ì„±ëœ ë¬¸ì¥\")\n",
    "    non_sarcastic_version: str = Field(description=\"í’ì ì—†ì´ ì‚¬ì‹¤ë§Œì„ ì „ë‹¬í•œ ë¬¸ì¥\")\n",
    "    sarcastic_explanation: str = Field(\n",
    "        description=\"í’ì ë¬¸ì¥ì´ ì™œ í’ìì¸ì§€ì— ëŒ€í•œ ì„¤ëª…\"\n",
    "    )\n",
    "    non_sarcastic_explanation: str = Field(\n",
    "        description=\"ë¹„í’ì ë¬¸ì¥ì´ ì™œ í’ìê°€ ì•„ë‹Œì§€ì— ëŒ€í•œ ì„¤ëª…\"\n",
    "    )\n",
    "\n",
    "\n",
    "# íŒŒì„œ ì„¤ì •\n",
    "parser = PydanticOutputParser(pydantic_object=SarcasmPair)\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ë° ì²´ì¸ ì„¤ì •\n",
    "prompt = load_prompt(\"sarcasm_prompt.yaml\", encoding=\"utf-8\")\n",
    "llm_4o = ChatOpenAI(temperature=0.5, model_name=\"gpt-4o\")\n",
    "llm_mini = ChatOpenAI(temperature=0.5, model_name=\"gpt-4o-mini\")\n",
    "chain_4o = prompt | llm_4o | parser\n",
    "chain_mini = prompt | llm_mini | parser\n",
    "\n",
    "# ì „ì²´ ë¦¬ë·° ë¡œë“œ\n",
    "df_all = pd.read_csv(\"negative_reviews.csv\")\n",
    "\n",
    "# ê¸°ì¡´ì— ì‚¬ìš©ëœ 500ê°œ ë¦¬ë·° ë¡œë“œr\n",
    "used_df = pd.read_csv(\"review_train.csv\")\n",
    "used_texts = set(used_df[\"context\"].unique())  # context ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±°\n",
    "\n",
    "# ì‚¬ìš©ë˜ì§€ ì•Šì€ ë¦¬ë·° í•„í„°ë§\n",
    "unused_df = df_all[~df_all[\"text\"].isin(used_texts)].reset_index(drop=True)\n",
    "\n",
    "# ìƒˆë¡œ ìƒ˜í”Œë§í•  100ê°œ ì¶”ì¶œ\n",
    "sample_df = unused_df.sample(n=100, random_state=123).reset_index(drop=True)\n",
    "\n",
    "# ìƒì„± ì‹¤í–‰\n",
    "results = []\n",
    "for idx, text in enumerate(sample_df[\"text\"]):\n",
    "    model_chain = chain_4o if len(text) <= 50 else chain_mini\n",
    "    try:\n",
    "        result = model_chain.invoke({\"original\": text})\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"context\": result.context,\n",
    "                \"response\": result.sarcastic_version,\n",
    "                \"label\": \"Sarcasm\",\n",
    "                \"explanation\": result.sarcastic_explanation,\n",
    "            }\n",
    "        )\n",
    "        results.append(\n",
    "            {\n",
    "                \"context\": result.context,\n",
    "                \"response\": result.non_sarcastic_version,\n",
    "                \"label\": \"Non-Sarcasm\",\n",
    "                \"explanation\": result.non_sarcastic_explanation,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if idx % 20 == 0:\n",
    "            print(f\"[í…ŒìŠ¤íŠ¸ì…‹ ì§„í–‰ ìƒí™©] {idx} / 100 ë¦¬ë·° ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[âš ï¸] ì˜¤ë¥˜ - {text[:30]}... â†’ {e}\")\n",
    "        continue\n",
    "\n",
    "# ì €ì¥\n",
    "jsonl_path = \"review_test.jsonl\"\n",
    "csv_path = \"review_test.csv\"\n",
    "\n",
    "with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in results:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "pd.DataFrame(results).to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"í…ŒìŠ¤íŠ¸ì…‹ ì €ì¥ ì™„ë£Œ: {jsonl_path} / {csv_path}\")\n",
    "print(f\"ì´ ìƒ˜í”Œ ìˆ˜: {len(results)}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79ea369",
   "metadata": {},
   "source": [
    "### ê²°ê³¼ìš”ì•½\n",
    "\n",
    "í† í° ì‚¬ìš©ëŸ‰: 74,968  \n",
    "ë¹„ìš©: $0.22  \n",
    "ëŸ°íƒ€ì„: 7ë¶„  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebb5310",
   "metadata": {},
   "source": [
    "## ğŸ»â€â„ï¸ ì¶”ê°€ ë°ì´í„° 250ê°œ\n",
    "ê²¹ì¹˜ì§€ ì•ŠëŠ” ë¶€ì • ë¦¬ë·° 125ê°œ (250 rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d10a1a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `_type` key found, defaulting to `prompt`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ì§„í–‰ ìƒí™©] 0 / 125 ë¦¬ë·° ì²˜ë¦¬ ì™„ë£Œ\n",
      "[ì§„í–‰ ìƒí™©] 25 / 125 ë¦¬ë·° ì²˜ë¦¬ ì™„ë£Œ\n",
      "[ì§„í–‰ ìƒí™©] 50 / 125 ë¦¬ë·° ì²˜ë¦¬ ì™„ë£Œ\n",
      "[ì§„í–‰ ìƒí™©] 75 / 125 ë¦¬ë·° ì²˜ë¦¬ ì™„ë£Œ\n",
      "[ì§„í–‰ ìƒí™©] 100 / 125 ë¦¬ë·° ì²˜ë¦¬ ì™„ë£Œ\n",
      "ì €ì¥ ì™„ë£Œ: Ikkyun_new.jsonl / Ikkyun_new.csv\n",
      "ì´ ìƒì„± ìƒ˜í”Œ ìˆ˜: 250ê°œ\n"
     ]
    }
   ],
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ\n",
    "import pandas as pd\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import load_prompt\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# .env ë¡œë“œ\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# ì¶œë ¥ ëª¨ë¸ ì •ì˜\n",
    "class SarcasmPair(BaseModel):\n",
    "    context: str = Field(description=\"ë¦¬ë·° ë‚´ìš©ì„ ìš”ì•½í•œ ë¬¸ë§¥\")\n",
    "    sarcastic_version: str = Field(description=\"í’ìì ìœ¼ë¡œ ì¬ì‘ì„±ëœ ë¬¸ì¥\")\n",
    "    non_sarcastic_version: str = Field(description=\"í’ì ì—†ì´ ì‚¬ì‹¤ë§Œì„ ì „ë‹¬í•œ ë¬¸ì¥\")\n",
    "    sarcastic_explanation: str = Field(\n",
    "        description=\"í’ì ë¬¸ì¥ì´ ì™œ í’ìì¸ì§€ì— ëŒ€í•œ ì„¤ëª…\"\n",
    "    )\n",
    "    non_sarcastic_explanation: str = Field(\n",
    "        description=\"ë¹„í’ì ë¬¸ì¥ì´ ì™œ í’ìê°€ ì•„ë‹Œì§€ì— ëŒ€í•œ ì„¤ëª…\"\n",
    "    )\n",
    "\n",
    "\n",
    "# íŒŒì„œ ì„¤ì •\n",
    "parser = PydanticOutputParser(pydantic_object=SarcasmPair)\n",
    "\n",
    "# í”„ë¡¬í”„íŠ¸ ë° ì²´ì¸ ì„¤ì •\n",
    "prompt = load_prompt(\"sarcasm_prompt.yaml\", encoding=\"utf-8\")\n",
    "llm_4o = ChatOpenAI(temperature=0.5, model_name=\"gpt-4o\")\n",
    "llm_mini = ChatOpenAI(temperature=0.5, model_name=\"gpt-4o-mini\")\n",
    "chain_4o = prompt | llm_4o | parser\n",
    "chain_mini = prompt | llm_mini | parser\n",
    "\n",
    "# ì „ì²´ ë¦¬ë·° ë¡œë“œ\n",
    "df_all = pd.read_csv(\"negative_reviews.csv\")\n",
    "\n",
    "# ê¸°ì¡´ ì‚¬ìš©ëœ context ë¡œë“œ (train + test)\n",
    "train_df = pd.read_csv(\"review_train.csv\")\n",
    "test_df = pd.read_csv(\"review_test.csv\")\n",
    "used_contexts = set(train_df[\"context\"].unique()).union(\n",
    "    set(test_df[\"context\"].unique())\n",
    ")\n",
    "\n",
    "# ì‚¬ìš©ë˜ì§€ ì•Šì€ ë¦¬ë·° í•„í„°ë§\n",
    "unused_df = df_all[~df_all[\"text\"].isin(used_contexts)].reset_index(drop=True)\n",
    "\n",
    "# 125ê°œ ìƒ˜í”Œë§\n",
    "sample_df = unused_df.sample(n=125, random_state=456).reset_index(drop=True)\n",
    "\n",
    "# ìƒì„± ì‹œì‘\n",
    "results = []\n",
    "for idx, text in enumerate(sample_df[\"text\"]):\n",
    "    model_chain = chain_4o if len(text) <= 50 else chain_mini\n",
    "    try:\n",
    "        result = model_chain.invoke({\"original\": text})\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"context\": result.context,\n",
    "                \"response\": result.sarcastic_version,\n",
    "                \"label\": \"Sarcasm\",\n",
    "                \"explanation\": result.sarcastic_explanation,\n",
    "            }\n",
    "        )\n",
    "        results.append(\n",
    "            {\n",
    "                \"context\": result.context,\n",
    "                \"response\": result.non_sarcastic_version,\n",
    "                \"label\": \"Non-Sarcasm\",\n",
    "                \"explanation\": result.non_sarcastic_explanation,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if idx % 25 == 0:\n",
    "            print(f\"[ì§„í–‰ ìƒí™©] {idx} / 125 ë¦¬ë·° ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[âš ï¸] ì˜¤ë¥˜ - {text[:30]}... â†’ {e}\")\n",
    "        continue\n",
    "\n",
    "# ì €ì¥\n",
    "jsonl_path = \"Ikkyun_new.jsonl\"\n",
    "csv_path = \"Ikkyun_new.csv\"\n",
    "\n",
    "with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for item in results:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "pd.DataFrame(results).to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"ì €ì¥ ì™„ë£Œ: {jsonl_path} / {csv_path}\")\n",
    "print(f\"ì´ ìƒì„± ìƒ˜í”Œ ìˆ˜: {len(results)}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c43214",
   "metadata": {},
   "source": [
    "### ê²°ê³¼ìš”ì•½\n",
    "\n",
    "í† í° ì‚¬ìš©ëŸ‰: 93,899  \n",
    "ë¹„ìš©: $0.29  \n",
    "ëŸ°íƒ€ì„: 7ë¶„  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-kr-G84JNwzB-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
